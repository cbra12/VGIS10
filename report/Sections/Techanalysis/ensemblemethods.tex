\section{Ensemble Methods}
Ensembles of classifiers is a popular method to increase the performance of many machine learning application and problems. In object detection, most current top performing systems are ensemble based. As mentioned in \sectionref{related}, this includes the top two performing methods on \gls{mscoco} that use Faster R-CNN ensemble with variants of ResNets.
This section will give an overview of ensemble methods in machine learning, including some popular methodologies. The section is largely inspired on the concepts from the comprehensive of ensemble methods in regards to methods and applications in \cite{ensemblebook}.
\\\\
One of the main goals of ensemble systems is to reduce the variance incorporated in the training process. By reducing variance a key issue that appears in the bias-variance trade-off problem in machine learning can be addressed. Bias is the error that arises from incorrect assumptions in the learning algorithm, high bias can result in an algorithm to miss important patterns in the given problem. Whereas variance is fluctuations in the training data, if there is a high amount of variance a model can overfit to random noise. Typically systems with low bias tend to have high variance and have models that are more complicated. Therefore, in ensemble methods a goal can be to have multiple classifiers that have a similarly low bias but are different in regards to the variance in training data. By combining these models the overall variance is reduced and hence accuracy improved. An example of having different variance is to train classifiers on different subsets of the data. By doing this the assumption is that the classifiers will make different errors on a given data point, however, by combining the classifiers the errors will be cancelled out by the increased strength from lower individual variance. Each classifier is considered an ensemble member in the overall system and can have be used in one of two settings. Firstly a member can be used for classifier selection, here each classifier is trained such that it is an expert in a local part of the feature space. Then during inference one of the members are selected to answer the problem based upon a distance measurement of the data in the feature space. Alternatively the members can be weighted according to their distances to the data and combined to produce a decision. The second way in which ensemble members can be used is in classifier fusion. With this method all members are trained over the entire feature space and fused to make a composite classifier. Due to differences in training, such as ordering of training data, the individual members are slightly different and fusing them leads to lower variance.

\subsection{Building an Ensemble System}
According to \cite{ensemblebook} there are three main strategies to building an ensemble system. Namely:

\begin{enumerate}
	\item Data sampling and selection: selection of training data for individual classifiers.
	\item Training member classifiers: specific procedure used for generating ensemble members.
	\item Combining ensemble members: combination rule for obtaining ensemble decision.
\end{enumerate}

The firstly strategy aims to increase the diversity of the individual ensemble members. A common method as mentioned earlier is to train the members on different subsets of the training data. Ideally the members should not give the same output for a given data point, otherwise the ensemble is superfluous. While important that members have their individual strengths in producing correct predictions that are different, even more important is that the members produce different errors. Again, if the members produce the same errors on a data point it is not possible to reach the correct outcome. However, if members produce different errors there is the potential that individual member error can be fixed when combining outcomes. The second strategy is in regards to how the members are trained. Variability in the ensemble can be reduced by using different strategies. Third is the final step in an ensemble system, how to combine the members. This step is dependent on the type of output from the classifiers. For example, a support vector machine may only return the class label without any additional information. In this instance the popular choice is to use a majority voting strategy where labels with the highest sum is taken as the ensemble output. However, if the output of the members is continuous, such as with accompanying confidence values in neural networks, more options are present. These can include multiple arithmetic methods such as mean, average, minimum, maximum and median.

