\section{Object Detectors}
This section will perform a technical analysis of the current primary \gls{cnn}-based object detectors.

\subsection{Faster Region-Convolutional Neural Network}



\subsection{Regional Fully-Connected Network}
One of the current leading object detection methods is the \gls{rfcn} \cite{rfcn}, which as mentioned in \sectionref{related}, takes a different approach to that of the region-based methods such as Faster R-CNN. The authors of \gls{rfcn} were inspired by the recent advances in \gls{fcn} classification networks, such as ResNets, and argue that the addition of the \gls{roi}-pooling layer in the Faster R-CNN pipeline is unnatural and adds computational complexity. The authors hypothesise that the reasoning behind this addition is due to the trade-off between using a classification approach in an object detection pipeline. A defining factor in object detection is that the method should be able to respect translation variance, that translation of an object inside an object proposal should given a good indication as to how well the proposal fits the object. Whereas classification is more translation invariant, as the shifting of an object in an image does not effect how the system returns it's output. The use of the \gls{roi}-pooling layer placed in between convolutional layers means that any convolutions after this point are not translation invariant as it is not region specific. Rather than using this popular feature extractor, \gls{rfcn} uses position-sensitive score maps computed by a bank of convolutional layers. The maps add translation variance into the detection pipeline by computing scores in relation to position information with respect to the relative spatial position of an object. A \gls{roi}-pooling layer is added after the score-maps, however, no convolutional operations are done after this point ensuring translation variance.
\\\\
The overall approach of the \gls{rfcn} also consists of the popular two-stages of region proposal and region classification. Region proposal is done using the \gls{rpn} from Faster R-CNN followed by the position-sensitive score maps and \gls{roi} pooling for region classification. The overall architecture of the \gls{rfcn} can be seen in \figref{rfcnarch}.


\begin{figure}[H]
  \centering
    \includegraphics[width=0.8\textwidth]{Figs/Techanal/rfcnarchi.png}
      \caption{Architecture of \gls{rfcn}. Region proposals are found using the \gls{rpn} followed by classification based on a bank of position-sensitive score maps.}
    \label{fig:rfcnarch}
\end{figure}

The added translation variance post finding proposals with the \gls{rpn} is done by producing a bank of $k^2$ score maps for each object category. Therefore, there are a total of $k^2(C + 1)$ maps, where $C$ is the number of object categories plus one for a background class. The number of $k^2$ maps is due to a $k \times k$ spatial grid representing relative positions. Typically $k = 3$, therefore, the nine score maps represent position-sensitive scores for a given object category.

\change[inline]{update score maps figure}
\begin{figure}[H]
  \centering
    \includegraphics[width=0.4\textwidth]{Figs/Techanal/scoremaps.png}
     \caption{PLACEHOLDER. Score maps.}
    \label{fig:scoremaps}
\end{figure}

Once the bank of score maps have been computed, position-sensitive \gls{roi}-pooling is found for region classification. Each individual $k \times k$ bin pools from its corresponding location in the relevant score map. For example, the top left bin pools from that position in the top-left score map and so on. The \gls{roi}-pool is computed using average pooling for each bin which can be seen in \figref{rfcnpooling}. The final decision for a given class is determined by a vote where each of the bins are averaged, producing a $(C+1)$-dimensional vector for each \gls{roi}.

\change[inline]{update score maps figure}
\begin{figure}[H]
  \centering
    \includegraphics[width=0.6\textwidth]{Figs/Techanal/rfcnpooling.png}
      \caption{PLACEHOLDER. Position-sensitive \gls{roi}-pooling operation for a given class.}
    \label{fig:rfcnpooling}
\end{figure}

\subsection{You Only Look Once}
YOLOv2 \cite{yolov2} is one of the current best performing single shot detectors, with results on par with more commonly used object detectors while being considerably faster at test time. YOLOv2 uses a different approach than the common 2-step method of region proposal and region classification seen in Faster R-CNN and \gls{rfcn} by directly computing class probabilities on each \gls{roi}. Some of the main distinct difference between YOLOv2 and region-based methods is the use of directly predicting bounding boxes, using a modified model, and altering how the priors for anchor boxes are computed during region proposals with the \gls{rpn}. The distinct differentiator for YOLO and YOLOv2 is that bounding boxes for a given object are predicted directly rather than predicting offsets to anchors with the \gls{rpn}. This is done by splitting the image into $S \times S$ grid cells, with each cell predicting $B$ bounding boxes. Each of the $B$ boxes predict a total of 5 values: $[t_x, t_y, t_w, t_h, t_o]$. Where $t_x, t_y$ are the coordinates of the centre of the given cell. The values $t_w, t_h$ are the width and height relative to the entire image. Finally, $t_\sigma$ is the confidence of how well the predicted box fits the ground truth. The location of the bounding box is determined by these values with respect to a given cell and the offset of the cell from the top left corner of the image $(c_x, c_y)$ and the size of the anchor box is $p_w, p_h$. Then the bounding box predictions are calculated by:

\begin{equation}
\begin{split}
  b_x = \sigma(t_x) + c_x \\
  b_y = \sigma(t_y) + c_y \\
  b_w = p_we^{t_w} \\
  b_h = p_he^{t_h}.
\end{split}
\end{equation}

Finally, the probability that the given bounding box fitting given the probability of their being an object is:

\begin{equation}  
  Pr(object) * \gls{iou}(b, object) = \sigma(t_o).  
\end{equation}

Each of the $S^2$ cells predicts $C$ conditional probabilities of it containing a given class and also being object by $Pr(Class_i \rvert Object)$. With the predicted bounding boxes and class probabilities calculated for each cell the final detections can be determined by adjusting a threshold based upon the calculation:

\begin{equation}
  Pr(Class_i \rvert Object) *  Pr(object) * \gls{iou}(b, object). 
\end{equation}

This process of using grid cells, bounding box prediction, cell class probabilities and final detections can be seen in \figref{yolomodel}.

\change[inline]{update figure}
\begin{figure}[H]
  \centering
    \includegraphics[width=0.8\textwidth]{Figs/Techanal/yolomodel.png}
      \caption{PLACEHOLDER. \gls{yolo} model.}
    \label{fig:yolomodel}
\end{figure}

As mentioned region proposals are found using the \gls{rpn} from Faster R-CNN \cite{fasterrcnn}. However, instead of using hand-picked priors for the anchor boxes, YOLOv2 proposed a method to learn more suitable sizes and aspect ratios. This is done by running k-means clustering on the annotated bounding boxes from the training set using a custom distance measurement. The custom measurement replaces Euclidean distance as these distances would create a bias due to more error on likely occurring on larger anchors. The custom distance measurement is designed for favourable \gls{iou} scores and is as follows:

\begin{equation}
  d(box, centroid) = 1 - IoU(box, centroid)
\end{equation}

where $box$ is the ground truth bounding box from the training set and $centroid$ is the predicted anchor box. By learning the priors YOLOv2 is able to use five anchor boxes at the same level of recall as the nine used in a typical \gls{rpn}.

YOLOv2 also goes against the grain in comparison to other state-of-the-art object detectors in regards to the choice of classification model. Rather than using the common networks such as VGG or ResNets, YOLOv2 propose their own 19 layer model dubbed Darknet-19. The model is of similar paradigm to VGG nets in that it uses mostly $3 \times 3$ convolutions and doubles the number of channels after pooling, which is also present in ResNets. But it is of considerably lower complexity than VGG-16 which consists of 15.3 billion \gls{flops}, with only 5.58 billion \gls{flops}. The baseline model has competitive results on ImageNet which can be seen in \tableref{darknetimagenet}. The baseline can be improved using standard data augmentations and also by initially training on $224 \times 224$ images followed by fine-tuning on $448 \time 448$, this is also shown as Darknet-19++ in \tableref{darknetimagenet}.

\begin{table}[]
\centering
\caption{ImageNet classification results for the Darknet-19 model.}
\label{tab:darknetimagenet}
\begin{tabular}{|l|l|l|}
\hline
Model                                                                                       & top-1 error (\%) & top-5 error (\%) \\ \hline
Darknet-19                                                                                  & 27.1             & 8.8              \\ \hline
Darknet-19++ & 23.5             & 6.7              \\ \hline
\end{tabular}
\end{table}

To aid in the detection of small objects the Darknet-19 model is also pre-trained on high-resolution images from ImageNet prior to training for object detection. Also fine-grained features are passthrough from an earlier layer when performing prediction. This gives features from a $26 \times 26$ feature map instead of the $13 \time 13$ size at the \gls{rpn}. Finally multi-scale training is also performed.

\subsection{Benchmark Results}
\begin{comment}
	RFCN
		train data union of voc 2007 trainval and voc 2012 trainval (07+12)
		test voc 2007 test set
		conv model ResNet-101
		slightly better on voc2007 (table 3)
		slight worse than Faster+++ (winner 2015) (table4)
			more bells/whistles
			rfcn only multiscale training
				train on coco - finetune on pascal
		faster
		depth - saturated at 101

    \subsection{Object Detection with ResNets}
    ResNets generalise well to computer vision tasks other than classification, such as object detection. The winning entries in 2015 from the authors of \cite{deepres} in \gls{ilsvrc} and \gls{mscoco} challenges for ImageNet detection, ImageNet localisation, COCO detection and COCO segmentation, were based upon the faster R-CNN framework with ResNets.
\end{comment}
This section will outline the results of the aforementioned \gls{cnn}-based object detectors on leading benchmarks such as \gls{pascalvoc} and \gls{mscoco}.