This chapter will include the discussion of the key areas of this thesis. This includes...

\section{Choice of Ensemble Factors}
The choice of object size and image quality as the deciding factors on how to create different ensemble members were made to address both challenges in object detection within object and image variations. The decision to use object size was clear as detectors results on benchmarks such as \gls{mscoco} showed that smaller objects are more difficult to detect. However, limited work has been done on image quality factors and object detection. Therefore, this was a more experimental part of the ensemble design. In regards to object size, the use of a trained \gls{rpn} is a strong and easy approach to use. However, the \gls{rpn} was trained to detect objects of all sizes. Rather it could have been possible to train an \gls{rpn} towards object sizes using the ground truth annotations such that each of the network ensemble were fed with region proposals from a better starting point. This could of course lead to issues in the ensemble as it would put a challenge on ensuring the ensemble members are combining results on the same object at test time. The choice of \gls{iqa} for the measurement of image quality may have a number of drawbacks. Firstly methods within \gls{iqa} attempt to estimate the subjective quality rather than objective. If image quality is a factor in object detection it would be due to the objective distortions present in an image, not that subjective evaluation of a person. Therefore, a method an objective method for measuring distortions in an image may be more appropriate. However, as the goal is ensemble methods is to reduce the variance in training the use of \gls{iqa} methods can still be suitable. Especially, if used in a systematic method as in this work.


\section{Choice of Object Detector}
The requirements set out in this work for the choice of the object detector was that it should be state-of-the-art \gls{cnn}-based. Three options were found, namely, Faster R-CNN, \gls{rfcn} and YOLOv2. Through both an analysis of the technical aspects and results of the three it was determined that a detector with ResNets as the backbone model should be used. Due to GPU limitations, the decision was made to use the \gls{rfcn}, however, any of the three could have been used. The overall goal of the thesis was to see if a \gls{cnn}-based object detector could be aided by ensemble methods chosen by robust-related challenges. Therefore, any of the detectors could have been used, as an assumption can be made that the benefits or limitations of the ensemble is equally applicable regardless of the choice.

\section{Training the Ensemble}
This section will discuss the process of training the ensemble members. This includes creating the data subsets for each member, the training of the \glspl{cnn} and.... \add[inline]{what is discussed}

\subsection{Creating the Data Subsets}
As mentioned, the aim of training multiple \glspl{rfcn} on different subsets of data was to decrease the overall variance in comparison to a model trained on all of the training data. Therefore, by measuring both object size and image quality factors on the 07++12 datasets the distributions of the data was the leading reason for how the subsets would be created. In all cases, apart from JPEG and JP2K compression, the distribution was heavily skewed towards lower numbers. As the requirement in the splitting of data was the subsets should have even number of ground truth examples this creating uneven datasets in regards to their variance. This choice leads to models within an ensemble factor which could vary greatly in terms of variance. There are potential positives and negatives to this. Firstly, by training two models the variance is already decreased as each model covers their own subset. However, one of these models has minimal variance and has to potential to be a powerful expert member. Issues with this are that the models are not trained under the same conditions which could lead to problems in the ensemble process. Which was attempted to be addressed by the weighted averaging combination process.
\\\\
Another issue when creating the datasets is ground truth examples for a given class may be skewed towards one of the two models. This can be seen in the examples of object size for the classes car (class 7) and cat (class 8) in \figref{classskew}. The distribution of ground truth object sizes is very different between the two classes. Cars in 07++12 tend to be of much smaller object size and is similar to the size distribution for all objects. Whereas, the cat class sizes are much more evenly distributed  However, the threshold for determining the split in data of 19,205.5 was found using the median size across all object sizes. Therefore, the number of cat training examples is much larger in this subset of larger ground truth examples. This could lead to the smaller resolution model not being able to generalise well towards images of this class.


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \center
        \includegraphics[width=\textwidth]{Figs/Conclusion/trainvalhist_class7.pdf}
        \caption{}\label{fig:}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \center
        \includegraphics[width=\textwidth]{Figs/Conclusion/trainvalhist_class8.pdf}
        \caption{}\label{fig:}
    \end{subfigure}
    \caption{Differences in distribution of ground truth object sizes for the classes car (a) and cat (b) in 07++12.}
    \label{fig:classskew}
\end{figure} 

A potential solution to both issues in the creation of the data subsets is to use augmented data. The use of such data is a common strategy in training \glspl{cnn}, with horizontal flipping of ground truths being used in this project. Data could however be augmented towards evening the differences in distributions in both cases. For example, by interpolating smaller ground truth objects to larger resolutions. However, an issue with this example is that interpolation methods can produce additional distortions in the form of artefacts if images are scaled to a large degree.

\subsection{Training \gls{rfcn} Members}
The training of the individual \gls{rfcn} ensemble members was kept constant such that the effect of different data sampling and selection could be evaluated. However, as seen in other works \gls{cnn}-based object detectors can be trained with different architectural considerations and after combined in an ensemble \cite{deepres}. Therefore, another option could have been to vary the \glspl{rfcn} such as with different filter sizes, loss functions or depth of the networks. These networks could then be tested on a validation set to find complementary networks for an ensemble.

\section{Ensemble Combination}
The two combination strategies of averaging and weighted averaging proved to be suitable to their own extent. For image size members the use of averaging provided best results, whereas for image quality members performed best with weighted averaging. It was a challenge to determine how to set weights in the weighted average approach. The choice of adjusting the weight based on how a given sample compared to the training distribution was chosen to take into account the difference in variance between sets of ensemble members. Again, this could also be addressed by augmented the data and the differencing in scaling weights between sets could be removed. An alternative to finding weights could be to test each ensemble member on a validation set and chosen depending on the performance.
\\\\
The calculation of weights were also chosen such that each set of the 5 ensemble factors had equal weight. However, this meant that when using all four image quality factors and the size factor in the ensemble the effective weight to the final decision is 80\% decided by image quality factors and only 20\% by resolution. Further tests are required to determine if resolution should be weighed higher at the compromise of image quality weights. Additionally, tests could be made within image quality factors to see if one is more important than another. For example, the models trained towards Gaussian blur may be more powerful than the JPEG members.

\section{Image Quality Distortions}
The image distortion types chosen was based upon the labelled data available in the LIVE image quality database. Other distortions could be have learnt with Deep IQA if datasets such as TID2013 \cite{tid2013} or CISQ \cite{cisq} were used. However, due to the training requirements of \gls{cnn} methods it was decided to only train towards the five distortions in LIVE. Future work could be made towards determining if other distortion types, such as the level of contrast, could be used in data sampling. There is the question if the distortions are at all present in the \gls{pascalvoc} dataset. While the distributions shown in \sectionref{iqaimp}, showed that the Deep IQA models found levels of distortion of each type apart from white noise some of these seem less likely than others. The distortion types of Gaussian blur, JPEG compression and JP2K compression are all plausible. Blur in images are quite common and the images were of JPEG type. However, the fast fading bit errors seems less probable, especially as many images in \gls{pascalvoc} are collected from Flickr. However, as mentioned earlier as long as the data sampling is consistent between training and testing the method can still be useful, despite the distortion possibly not being present.

\section{Ensemble Member Experts}
The intuition when determining how to train the \glspl{rfcn} were that each network would become an expert on their respective subset of the training data. While it was found that this was the case for object sizes, this did not prove to be the case for the image quality members. Generally the members trained towards to quality distortions performed 3-4\% worse in \gls{ap} in comparison to the model trained on all of the data. This could indicate the the learned image quality distortions are not as large of a factor in detection as object size is. Interestingly, when the image quality members were combined in an ensemble the performance increased by roughly 3\%, showing that the 8 models complement each other well. 
\\\\
While the individual object size networks performed well on their own testing subsets, when combined into an ensemble the performance was worse than the baseline model, regardless of the combination strategy. This is a curious result and may have something to do with potential inaccuracies in region proposals with the \gls{rpn}. The combination strategy is based upon the area of the given region proposal. However, this split of the data was found using the ground truth annotations. Therefore, if the region proposals do not fit the object well the incorrect model may be weighed higher. This is a possibility as bounding-box regression is performed in the latter stages of the \glspl{rfcn}, potentially fixing poor proposals. 

\section{Future Work}