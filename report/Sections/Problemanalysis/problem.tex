\begin{comment}
	%%%%Overview%%%%
	- summary of object class recognition
		- reit challenges from introduction
		- reit of goals - categorisation/localisation
		- one class or multi?
		- show figure 2
			- discuss differences and similarities 
		- general implementation pipeline
	- key challenges - cite robustness/scalability
	- intro to older (non-CNN/deep) methods
		- find from survey
		- eg hand picked feature extractors/SVMS
		- sliding window approaches
		- DPM
	- CNN based methods - taken over since Alexnet (2012) on Imagenet
		- was this classification
		- extended to object detection (use classification network)
		- increase in complexity with CNNs, increase in time per window
			- overcome with region proposals
		- became a 2 step process
			1. find a number of region proposals where an object may be situated
			2. determine if a given proposal is a given pre-defined class
		- overview of proposal methods
			- grouping & window scoring
				- summary of both
			- should be accurate & have high repeatability (see why in dollar cite)
		- pipeline therefore:
			- with a given region proposal method apply deep-based classifier on regions
			- multiple current SOTA do this or similar
				- R-CNN, Fast R-CNN, Faster R-CNN (accurate but not real-time)
				- SSD, YOLOv2/YOLO9000, R-FCN 
					- alternatives to R-CNN 
						- often use some of the same methods (RPN)
				- best instance segmentation COCO 2016 - 2nd best bbox COCO 2016 (2016????)

- important that object detector is translation-invariant
by nature image-level classification favors translation invariance. A shift of an object inside an image should be indiscriminative
However, object detection needs to localise representations that are translation-variant to an extent. Translation of an object inside a candidate box should produce meaningful responses for describing how good candidate box overlaps object
CITE: R-FCN 2016

\end{comment}

This chapter will outline object detection and it's key challenges. This includes aspects within robustness, computational-complexity and scalability. Once completed the key works within object detection will be analysed, both current state-of-the-art and notable older methods.

\section{Object Detection}
As mentioned in \sectionref{intro}, object detection consists of two larger tasks; classification and localisation. Depending on the problem at hand, object detection can be split into two categories. If only a single class is of interest, such as detecting a specific traffic sign, the object detection task is denoted as class-specific detection. Whereas, the more general case when multiple classes are of interest in an image is denoted as multi-class detection \cite{zhang}. Key challenges such as \gls{pascalvoc}, ImageNet, and \gls{mscoco} are of the latter task. This thesis will be within the multi-class detection domain and take these challenges into account when analysing related works in \sectionref{sota} and determining the algorithm to be implemented and evaluated in \sectionref{methodoverview}. An analysis of these key challenges is done in \sectionref{challenges}. The goal of a detector is to output a list of labels from a predefined list of categories indicating which objects are present and where they are located in an image. Object detection has a number of related fields which share to common goal of categories relevant objects. This can be seen in \figref{objfields}. In all four instances the goal is to categorise the two objects person and skateboard, however, the difference lies in the level of localisation precision. In \figref{objcat}, object categorisation aims to only classify the objects in the image without providing any indication as to where the objects are located. Object class detection in \figref{objbb}, localises the classified objects with the use of bounding-boxes, where ideally the bounding-boxes are placed as tightly around the given object as possible. Figure-ground segmentation in \figref{objfig}, indicates localisation with a lasso outline around the objects. Finally, in \figref{objseg}, semantic-segmentation localises objects at a pixel-level classifying each pixel that is related to the given object. 


\add[inline]{correct section refs to above}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \center
        \includegraphics[width=\textwidth]{Figs/Problem/objfieldscat.png}
        \caption{}\label{fig:objcat}
    \end{subfigure}
    \begin{subfigure}[b]{0.2\textwidth}
        \center
        \includegraphics[width=\textwidth]{Figs/Problem/objfieldsbbox.png}
        \caption{}\label{fig:objbb}
    \end{subfigure}
    \begin{subfigure}[b]{0.2\textwidth}
        \center
        \includegraphics[width=\textwidth]{Figs/Problem/objfieldsfiguresegmentation.png}
        \caption{}\label{fig:objfig}
    \end{subfigure}
    \begin{subfigure}[b]{0.2\textwidth}
        \center
        \includegraphics[width=\textwidth]{Figs/Problem/objfieldssegmentation.png}
        \caption{}\label{fig:objseg}
    \end{subfigure}
    \caption{Example of vision tasks related to object detection. All tasks have the common goal of categorising predefined objects. Methods are: object categorisation (a), object class detection (b), figure-ground segmentation (c), semantic Segmentation (d). Image and class labels taken from \gls{mscoco} \cite{mscoco}.}
    \label{fig:objfields}
\end{figure} 

A more recent example of segmentation is that of instance segmentation. Instance segmentation varies to semantic segmentation in that individual instances of objects are classified as such. If multiple instances of the same object is present, such as an image of a crowd with many people, in semantic segmentation all people will be given the same label as one large group. However, in instance segmentation the people are still given the same label but individual instances of a person is also found. This area of research within segmentation is relatively new, however, is beginning to become more popular in comparison to semantic segmentation. For example, the \gls{mscoco} segmentation challenge which has been held in 2015 and 2016 only accepts instance segmentation entries.


\section{Main Challenges}
The challenges of object detection can be split into two groups as per \cite{zhang}:

\begin{itemize}
	\item Robustness-related.
	\item Computational-complexity and scalability-related.
\end{itemize}

The following sections will outline the above.

\subsection{Robustness-related Challenges}

Robustness-related refers to the challenges in appearance within the both of intra-class and inter-class. Intra-class is the differences in appearance of objects which are of the same class. For example as seen in \figref{intra_ex}, all of the images belong to the superclass chair from the ImageNet training set \cite{imagenet}, however, vary greatly in their overall appearance. 

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \center
        \includegraphics[width=\textwidth]{Figs/Problem/chair1.jpeg}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.2\textwidth}
        \center
        \includegraphics[width=\textwidth]{Figs/Problem/chair2.jpeg}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.2\textwidth}
        \center
        \includegraphics[width=\textwidth]{Figs/Problem/chair3.jpeg}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.2\textwidth}
        \center
        \includegraphics[width=\textwidth]{Figs/Problem/chair4.jpeg}
        \caption{}
    \end{subfigure}
    \caption{Examples of intra-class appearance variation. All images have the label chair in the ImageNet training set \cite{imagenet}.}
    \label{fig:intra_ex}
\end{figure} 

An object detection system must be able to learn \add[inline]{explanation that typically obj detection is supervised learning} the appearance variations that can occur intra-class. These variations can be categorised into two types as per \cite{schroff}:

\begin{itemize}
	\item Object variations.
	\item Image variations.
\end{itemize}

Object variations consist of appearance differences between instances of colour, texture, shape, and size. Image variations are differences not related to the object instances themselves but rather consist of conditions such as lighting, viewpoint, scale, occlusion, and clutter. Based upon these conditions the task of both classifying a given object as a given class but also differentiating the potentially largely varying objects into the same class challenging.
\unsure[inline]{unsure if to add explanation of structured and unstructured classes}

Robustness-related challenges can also occur with inter-class appearance differences. This refers to the differences between objects that are regarded as different categories. Challenges arise in scenarios where an object detector must decide if an instance is between classes that are very similar. For example using images and their respective classes from ImageNet \cite{imagenet}, in \figref{inter1} and \figref{inter2} the differences between the two examples are very similar, however, their class labels are different. In \figref{inter1a} and \figref{inter1b} the class labels are mini-bus and delivery truck respectively. In \figref{inter2a} and \figref{inter2b} the labels are white wolf and German shepherd. 

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \center
        \includegraphics[width=\textwidth]{Figs/Problem/minibus.jpeg}
        \caption{}\label{fig:inter1a}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \center
        \includegraphics[width=\textwidth]{Figs/Problem/deliverytruck.jpeg}
        \caption{}\label{fig:inter1b}
    \end{subfigure}
    \caption{Examples of inter-class appearance variation. Both images are from the ImageNet training set \cite{imagenet} and have the labels mini-bus (a) and delivery truck (b).}
    \label{fig:inter1}
\end{figure} 

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \center
        \includegraphics[width=\textwidth]{Figs/Problem/wolf.jpeg}
        \caption{}\label{fig:inter2a}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \center
        \includegraphics[width=\textwidth]{Figs/Problem/germanshepherd.jpeg}
        \caption{}\label{fig:inter2b}
    \end{subfigure}
    \caption{Examples of inter-class appearance variation. Both images are from the ImageNet training set \cite{imagenet} and have the labels White wolf (a) and German shepherd (b).}
    \label{fig:inter2}
\end{figure} 

It should be noted that this is a task-specific if inter-class appearance similarities is a problem or not. It can be argued that both the examples in \figref{inter1} and \figref{inter2} can be grouped into a larger superclass label if the given task does not require training of a model to such granularity. In both examples the classes stated are of the lowest class available in the overall hierarchy. ImageNet has labels available for each image along a larger array of classes and sub-classes. \figref{inter1_hierachy} visualises the granularity possible where both images belong to the superclass animal.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.25\textwidth}
        \center
        \includegraphics[width=\textwidth]{Figs/Problem/wolf.jpeg}
        \caption{}\label{fig:interhier1a}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\textwidth}
        \center
        \includegraphics[width=\textwidth]{Figs/Problem/wolf_hier.png}
        \caption{}\label{fig:interhier1b}
    \end{subfigure}
        \begin{subfigure}[b]{0.25\textwidth}
        \center
        \includegraphics[width=\textwidth]{Figs/Problem/germanshepherd.jpeg}
        \caption{}\label{fig:interhier2a}
    \end{subfigure}
    \begin{subfigure}[b]{0.20\textwidth}
        \center
        \includegraphics[width=\textwidth]{Figs/Problem/german_hier.png}
        \caption{}\label{fig:interhier2b}
    \end{subfigure}
    \caption{Visualisation of the hierarchy of potential classes for two examples in the ImageNet training set \cite{imagenet}.}
    \label{fig:inter1_hierachy}
\end{figure}

\subsection{Computational-complexity and Scalability-related Challenges}
The second challenge as per \cite{zhang} is related to the potential scale of object detection. When deciding on which type of model to use it must be complex enough to be able to capture the previously mentioned challenges both in inter- and intra-class. On top of this there is an extremely large number of potential classes in object detection. If the aim is to train a model to classify between an extreme number of classes then naturally a large number of images are also needed for each category. The large number of images need also to be representative enough in training to capture the necessary visual features to generalise on non-training images. In 2016 the ImageNet object detection challenge there is a total of 200 object categories, with 456,567 images comprising the training set. Therefore, a complex enough model is needed in order to learn and generalise on such a dataset but this of course places high requirements on the amount of training needed.

Issues can also arise over time when designing an object detection system.  Over time the visual appearance of an object can change, which is very difficult to take into account when training a model. For example, the visual appearance of televisions have changed greatly in the past century. If a system were only to be trained on images from an earlier time period it may not be able to generalise on new instances. Therefore, it is important that a model is able to be updated as the appearance of objects change. On top of this, new categories of objects can come to fruition which may needed to be added to a model. 

\section{Implementation Outline}
As per \cite{zhang} the steps in the general pipeline for an object detection system is as follows:

\begin{enumerate}
	\item Find all possible object regions in the image.
	\item Determine if the regions correspond to any of the predefined categories.
	\item Evaluate all responses from step 2 to determine final detections.
\end{enumerate}


