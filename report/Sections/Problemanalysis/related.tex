\begin{comment}
	- CNN approaches
	- R-CNN - fast - faster
	- SSD
	- R-FCN
	- YOLO (speed)	

	- outline overall CNN methods
		- R-CNN
		- single-shot
	- cites to specific implementations within each
		- inspiration from imagenet / ms coco


	- history from contextual priming and feedback for faster RCNN
		- manually designed features (13, 22) to convnet features (29, 37, 44, 68)
		- from sliding window approaches (22, 77) to region proposals (228, 29, 32, 63, 78)
		- from pipeline RCNN to end-to-end Fast and Faster 
\end{comment}

\section{Related Work}
One of the first methods to show that \glspl{cnn} could significantly improve object detection was that of R-CNN \cite{rcnn}. The method obtains the name R-CNN based upon a \gls{cnn} is used on regions of the image. Many earlier object detection approaches were used in a sliding window fashion testing all areas of an image. This can lead to a huge amount of potential testing windows especially if the object detection is done at a multitude of different scales. The method was heavily inspired by the AlexNet model that started the deep learning renaissance in 2012 by winning classification challenge in the \gls{ilsvrc}. The authors of R-CNN aimed to show that the advances in classification with a model such as AlexNet could also be done in object detection. In R-CNN the model is used as a feature extractor from which a class-specific linear \gls{svm} can be trained on top of. The AlexNet-based feature extractor is firstly pre-trained on a large dataset designed for classification, in this case the training set from \gls{ilsvrc} 2012. This pre-trained model is then adapted to the new domain of object detection by fine-tuning the model accordingly. In this instance the authors fine-tuned warped training instances from the \gls{pascalvoc} dataset. The AlexNet model was also altered to classify the 20 classes present in \gls{pascalvoc} rather than the 1000 classes in \gls{ilsvrc}. The pipeline of the R-CNN is split into 3 modules:

\begin{enumerate}
	\item Region proposals.
	\item Feature extraction.
	\item Class-specific linear \glspl{svm}.
\end{enumerate}

In this first module, region proposal, there is a large number of choices of methods to produce a suitable number of windows in comparison to a sliding window approach. R-CNN is agnostic to the region proposal method chosen and in the original work SelectiveSearch \cite{selectivesearch} is used. Module two, as explained earlier, is the use of a \gls{cnn} as a feature extractor. This is in the form of a 4096-dimensional feature vector from the domain-specific \gls{pascalvoc} trained AlexNet model. These feature vectors are used in the third module, class-specific linear \glspl{svm}. In the case of \gls{pascalvoc} a total of 21 \glspl{svm} are trained, one for each of the 20 classes in the challenge and one for a background class. The training of the \glspl{svm} is done by forward propagating a large number of both positive and negative region proposals found with SelectiveSearch and storing each 4096-dimensional feature vector to disk. After this the appropriate labels are applied to each vector and a linear \gls{svm} is optimised for the 21 classes. At test time, for a given image SelectiveSearch is used to produce around 2000 proposals. Each of the proposals are propagated through the network to extract their respective feature vectors. Each feature vector is then tested against every \gls{svm} to produce a score for each class. Finally greedy \gls{nms} is applied to remove overlapping detections. The approach outlined in R-CNN produced a significant improvement in object detection with an improvement of roughly 13\%, compared to previous state-of-the-art methods, to an overall 53.7\% \gls{map} on the \gls{pascalvoc} 2010 test set. Similar results were also found on the \gls{pascalvoc} 2011/12 set with \gls{map} of 53.5\%. Despite the significant improvements with a \gls{cnn}-based method on region proposals there are still issues with the R-CNN. Firstly, the testing time per image is very slow at roughly 47 seconds on an Nvidia K40 GPU. Also extracting features for each proposal in order to train the \glspl{svm} takes a large amount of disk space and may not be feasible on all hardware configurations. Finally, as the R-CNN is made up of 3 modules the training is done in a multi-stage manner rather than end-to-end. Therefore, the loss calculating when optimising the \glspl{svm} are not used to update the \gls{cnn} parameters.
\\\\
The R-CNN method was improved the following year with Fast R-CNN \cite{fastrcnn} and aimed to improve speed and accuracy. One of the significant changes is that the detection training done is now end-end rather than in the multi-stage pipeline in R-CNN. Due to this the large requirements of disk space due to feature caching is no longer required. The Fast-RCNN method takes both an image and a set of pre-computed object proposals. A \gls{cnn} forward propagates the entire image, rather than individual proposals in R-CNN, through several convolutional and max-pooling layers to produce a feature map. Features are extracted for each proposal in their corresponding location in the computed feature map with a \gls{roi} pooling layer. The \gls{roi} feature is calculated by splitting the $h \times w$ proposal into $H \times W$ sub-windows of size $h/H \times w/W$. Where $h$ and $w$ is the height and width respectively of a proposal. $H$ and $W$ are hyper-parameters specifying the fixed spatial extent of the extracted feature. Each sub-window has max-pooling applied and with the resulting value being placed in the corresponding output cell. Once the \gls{roi} pooling layer has been applied to a pre-computed object proposal the forward pass continues through two fully-connected layers followed by two sibling output layers. The sibling outputs are a softmax classification layer that produces probabilities for the object classes and another layer for bounding-box regression. These two layers replace the respective external modules in R-CNN and make it possible to train the entire detection network in a single-stage. As in R-CNN, pre-training a \gls{cnn} on a large classification dataset and fine-tuning towards detection and a specific object classes is done in a similar fashion. In R-CNN, the only deep network used was AlexNet \cite{alexnet}, however, in Fast R-CNN the authors experiment with networks of different size. It was found that the deeper network VGG-16 \cite{vgg16} for computing the convolutional feature map gave a considerable improvement in \gls{map}. For a fair comparison of results against R-CNN, its \gls{cnn} was the same pre-trained VGG-16 network. The \gls{map} results on \gls{pascalvoc} 2007, 2010, and 2012 compared against R-CNN can be seen in \tableref{fastresults}. 

\begin{table}[]
\centering
\caption{A comparison of R-CNN and Fast R-CNN \gls{pascalvoc} \gls{map} results on the test set from 2007, 2010, and 2012.}
\label{tab:fastresults}
\begin{tabular}{|l|l|l|l|}
 \hline
           & 2007 & 2010 & 2012 \\ \hline
R-CNN      & 66.0 & 62.9 & 62.4 \\ 
Fast R-CNN & \textbf{66.9} & \textbf{66.1} & \textbf{65.7} \\ \hline
\end{tabular}
\end{table}

However, as the name Fast R-CNN implies the main improvement is the speed in respect to both training and testing. By computing a convolutional feature map for an entire image rather than per object proposal the number of passes in the network is lowered significantly. Also by makign the detection training end-to-end with the two sibling layers lowers the training time needed. An overview of time spent training and testing both Fast R-CNN and R-CNN with a VGG16 \gls{cnn} can be seen in \tableref{fastspeed}.

\begin{table}[]
\centering
\caption{Speedup between R-CNN and Fast R-CNN in regards to both training and testing. Both methods are train a VGG16 network for object detection.}
\label{tab:fastspeed}
\begin{tabular}{|l|l|l|}
\hline
                          & R-CNN & Fast R-CNN \\ \hline
Train time (hours)        & 84    & \textbf{9.5}        \\ 
Train speed-up             & 1x    & \textbf{8.8$\times$}       \\ \hline
Test time (seconds/image) & 47.0  & \textbf{0.32}       \\ 
Test speed-up              & 1x    & \textbf{146$\times$}       \\ \hline
\end{tabular}
\end{table}

While Fast R-CNN provided improvements in both accuracy and speed, the increase in speed is only in relation to the actual object detection and assumes that the region proposals are pre-computed. Therefore, there is still a significant bottleneck per image as a region proposal method can typically take a couple of seconds. 
\\\\
The region proposal bottleneck was addressed in the third iteration of the R-CNN network with Faster R-CNN \cite{fasterrcnn}. In this method it was shown how to compute region proposals with a deep \gls{cnn} with a part of the network called \gls{rpn}. A \gls{rpn} shares the convolutional layers and feature map used for computing features with \gls{roi} pooling in Fast R-CNN. As this deep network is already being computed on the entire image for the classification pipeline the added time for proposals using the \gls{rpn} is negligible (10ms) in comparison to a method such as SelectiveSearch. Apart from the change in how region proposals are computed there is no change in comparison to Fast R-CNN. 
An \gls{rpn} takes the convolutional feature map as input and returns a number of object proposals. Each proposals is fed into two sibling layers, similar to that in Fast R-CNN, one layer scoring how likely to be an object or background and another performing bounding-box regression. The proposals are found through a method denoted as anchors. At each sliding-window location $k$ proposals are with anchors that are user-defined reference boxes for how an object proposal may be formed. The anchors can be built based upon scale and aspect ratio altering the size. In the original work three different scales and three aspect ratios are built, yielding a total of $k$ = 9 anchors at each sliding window position. These anchors are then placed on the feature map and the sibling layers calculate the likelihood of an object and regress the anchor as necessary.  
Once the proposals have been found with the \gls{rpn} these are placed on the same convolutional feature map as earlier and the rest of the pipeline is identical to Fast R-CNN, classifying and regressing bounding-boxes with another set of sibling layers. As the only change is the addition of computing proposals in the network with \gls{rpn}, the results are similar in respect to \gls{map}. Only a slight improvement in made on \gls{pascalvoc} 2007 and 2012, from 66.9\% to 69.9\% and 65.7\% to 67.0\% respectively. However, the main contribution to the work is the speed-up of the entire object detection pipeline as the object proposal time is now minimal. On average processing an image on \gls{pascalvoc} 2007 with an Nvidia K40 with Fast R-CNN including proposals took 2 seconds per image. While in Faster R-CNN with the same hardware takes 0.2 seconds per image. A speed-up of 10$\times$ from Fast R-CNN to Faster R-CNN and 250$\times$ from the original R-CNN. The Faster R-CNN methods has also proved to be the foundation for the winning entry in multiple detection challenges including \gls{mscoco}. The results for this challenge with a VGG-16 model for Fast R-CNN were 35.9\% \gls{map}@0.5 \gls{iou} and 19.7\% \gls{map}@[0.5, 0.95]. Faster R-CNN improved this to 42.7\% \gls{map}@0.5 and 21.9\% \gls{map}@[0.5, 0.95].
\\\\
Much of the recent work within object detection has been based upon the Faster R-CNN framework. This is exemplified by looking at the \gls{mscoco} detection leaderboard, with 15 of the 21 approaches being Faster R-CNN related in some way. Firstly, the winner of the \gls{mscoco} 2015 and \gls{ilsvrc} 2015 detection challenge was with the use of deep residual networks \cite{deepres}. As is well known with \glspl{cnn}, deeper networks are able to capture richer higher-level features and the authors showed that this is also beneficial in the object detection domain. In \cite{deepres} an ensemble of three deep residual networks with 101 layers was trained for object detection and another ensemble of three used for region proposals with the \gls{rpn} while being based on the Faster R-CNN framework. In addition to the ensemble, the winning entry also added box refinement, global context, and multi-scale testing to the Faster R-CNN.
\\\\
The current leading method on \gls{mscoco} is an extension of the previously explained \cite{deepres}. This method dubbed G-RMI on the \gls{mscoco} leaderboard \cite{cocolead} is an ensemble of five deep residual networks based upon ResNet \cite{deepres} and Inception ResNet \cite{incepres} feature extractors. No work has been published yet on G-RMI, however, a short explanation of the entry is included in a survey paper \cite{speedacc} from the winning authors. The approach was to train a large number of Faster R-CNN models with varying output stride, variations on the loss function, and different ordering of the training data. Based upon the collection of models, five were greedily chosen based upon performance on a validation set. While performance on the models were important, the models were also chosen such that they were not too similar. It should also be noted that apart from the ensemble of models, the entry did not include any extras such as multi-scale training, box refinement, or global context.
\\\\
Another variant is that of MultiPath \cite{multipath}, placing second in \gls{mscoco} 2015. Which aimed to address the many small objects present in \gls{mscoco} by modifying Fast R-CNN. Firstly, rather than only having a single classification head, MultiPath has four. Each classification head observes different scaled regions around the \gls{roi} which aims to add context around the object. The output of each of the four are concatenated for classification and regression. Also MultiPath uses skip connections. The \gls{rpn} and consequent \gls{roi}-pooling in Faster R-CNN and Fast R-CNN is only performed once at a number of convolutional layers. At which point the input image has been down-sampled a number of times, therefore, small objects are potentially not represented very well any more. With the use of skip connections higher-resolution features from earlier convolutional layers can be added giving the \gls{rpn} and classier information about smaller objects.
\\\\
\gls{ion} \cite{ion} also adds contextual and multi-scale information on top of Fast R-CNN. \gls{ion} was the third place entry in \gls{mscoco} 2015. The multi-scale information is also added with skip connections. Whereas global context is added through the use of \glspl{rnn} passing information about the image both vertically and horizontally.
\\\\
Global context has also been added to the Faster R-CNN framework in \cite{contextprim} with the use of semantic segmentation as a form of top-down information. A segmentation module is augmented onto the framework and is calculated using the same initial convolutional layers as Faster R-CNN. The segmented result is then added after the \gls{rpn} and \gls{roi}-pooling is performed on both the convolutional layers and corresponding \gls{roi} segmentation area.
\\\\
Additional work on adding finer details for smaller objects with Faster R-CNN was performed in \cite{beyondskip} who aimed to improve skip connections with additional top-down information. While skip connections is useful method for finding higher-resolution features, the authors argue that with \gls{tdm} features are taken from an appropriate lower layer. \gls{tdm} is incorporated into the Faster R-CNN framework and can be trained along side it. 
\\\\
The use of hard example mining was conducted in \cite{ohem}. In this work the problem of small objects in \gls{ilsvrc} and \gls{mscoco} is also addressed. The authors present a method for training a Fast R-CNN object detector for these objects with \gls{ohem}. Inspired by bootstrapping, with \gls{ohem} a modification is made to \gls{sgd} training by selecting \glspl{roi} that the network currently has a high loss on and backpropagating accordingly. 
\\\\
The methods covered so far have all followed a region-based paradigm of first finding a selection of object proposals and second classifying these into one of the appropriate classes while also regressing bounding-boxes. These methods can be computationally expensive and therefore recent work has attempted to combine the two steps into a single feed-forward \gls{cnn}. These methods can be denoted single shot object detectors, with he most recent approach is that of \gls{ssd} \cite{ssd} and is the first deep approach that does not resample pixels of features to perform object detection such as the convolutional feature maps and \gls {roi} pooling in region-based methods \cite{fastrcnn} \cite{fasterrcnn}. Rather convolutional feature layers are added to the end of a network and a small convolutional filter (predictor) is applied on these for simultaneous localisation and classification. The truncated layers become progressively smaller and allow \gls{ssd} to find objects at multiple scales. The predictors used on these layers are of pre-determined size and aspect ratio similar to the anchor boxes used in Faster R-CNN \cite{fasterrcnn}. The additional layers and predictors can be added to any classification-based \gls{cnn} and \gls{ssd} test using the standard VGG-16 network. On top of this the authors train two separate instances of the network, one for low-resolution input \gls{ssd}300 (300$\times$300) and one for high-resolution \gls{ssd}512 (512$\times$512). Overall the higher-resolution network performs best with 1-2\% improvements in comparison to Faster R-CNN on \gls{pascalvoc} 2007 and \gls{mscoco} test-dev 2015. In terms of speed there is a considerable difference, the authors found Faster R-CNN on average took 0.14 s/image, \gls{ssd}300 0.02 s/image, and \gls{ssd}512 0.05 s/image on \gls{pascalvoc} 2007 testing with a Titan X GPU. 
\\\\
One of the original single shot \gls{cnn}-based methods for object detection was OverFeat \cite{overfeat} with a sliding-window approach. Methods such as OverFeat have not recently been as popular due to deeper and more powerful networks being too computationally expensive to be run across the entire image at multiple scales. However, at the time OverFeat won the \gls{ilsvrc} 2013 localisation challenge using an altered AlexNet \cite{alexnet} \gls{cnn}. The main alteration was a regression layer for added accuracy in localisation.
\\\\
A precursor to \gls{ssd} was that of MultiBox \cite{multibox1} and the improved version in \cite{multibox2}. Again the goal was to directly predict the bounding-box of an object directly with a \gls{cnn} for a given class. The MultiBox method was originally designed to prove that object proposals with a \gls{cnn} could be an improvement of hand-engineered methods such as SelectiveSearch \cite{selectivesearch} in R-CNN \cite{rcnn} and Fast R-CNN \cite{fastrcnn}. MultiBox is similar to the \gls{rpn} in Faster R-CNN \cite{fasterrcnn} where object locations are predicted on a grid with a number of default predictions of different sizes. Additionally, MultiBox ranks the proposals according to a loss in relation to both the confidence of being an object and location of the bounding-box. With this ranking MultiBox is able produce and classify only 15 proposals per image while being competitive to other methods such as R-CNN at the time.
\\\\
Another \gls{cnn} method that only uses a single network is \gls{yolo} \cite{yolo} and it's successor YOLOv2 \cite {yolov2}. In the original \gls{yolo} method, a single shot approach was taken by producing bounding box locations and class scores from a fixed grid in an image. Various combinations of these grids are used as potential bounding-boxes. However, in \gls{yolo} the accuracy of the localisation was poor and this was addressed in YOLOv2 with a number of changes. Firstly, the \gls{rpn} from Faster R-CNN \cite{fasterrcnn} was adapted with this use of anchor boxes. But rather than using pre-determined anchor sizes k-means clustering is run on the training set to determine what appropriate sizes. Also to address the issue of small objects not being represented in the convolutional feature map in the \gls{rpn}, additional features are added from an earlier convolutional layer. Other improvements to YOLOv2 include batch normalisation, multi-scale training, and high-resolution inputs. Overall the method produces competitive results against approaches such as Faster R-CNN and \gls{ssd}. But the main improvement is in speed, where at inference time is up to 182$\times$ and 5$\times$ faster than Faster R-CNN and \gls{ssd} respectively.
\\\\
Recently a newer approach to region-based methods, such as Faster R-CNN, has been proposed with the use of \glspl{fcn}. The authors argue that in region-based methods the act of cropping features from \glspl{roi} in the same layer adds an unnatural condition. There has been an issue in the two step pipeline in region-based methods, as the classification is translation-invariant, whereas detection in translation-variant. Due to this difference region-based methods have been adjusted towards the invariant properties of classification by pooling features and classifying them. However, \cite{rfcn} argue that translation-variant representations are important in object detection as the position of an object inside a \gls{roi} can provide meaningful information. Therefore, \cite{rfcn} present their fully convolutional approach with \gls{rfcn}. The overall approach is similar to that used in region-based methods such as \cite{rcnn}, \cite{fastrcnn} and \cite{fasterrcnn}. First compute \glspl{roi} using a region proposal method and second perform classification on these regions. \gls{rfcn} uses the \gls{rpn} from Faster R-CNN \cite{fasterrcnn} for class-agnostic \gls{roi} computation. However, rather than extracting features with \gls{roi}-pooling, fully convolutional position-sensitive score maps are computed. The score maps are split up to represent a relative position in a $k\times k$ grid, with each cell presenting information relative to the spatial position of an object. For example, the upper-left cell represents scores that pixels are present at that relative position to the object. A bank for position-sensitive score maps are found for each class, generating a total of $k^2(C+1)$ where $C$ is the number of classes plus a background class. After computing the bank, the \gls{rfcn} computes a position-sensitive \gls{roi}-pooling layer for each class. For each \gls{roi} found with the \gls{rpn} each cell aggregates the response from the appropriate score map from the bank of maps. While the ordering and methodology of \gls{roi}-pooling is different in \gls{rfcn} to that of Faster R-CNN the same backbone \gls{cnn} can be used. In this experiments conducted by the authors a ResNet-101 network is chosen. Overall \gls{rfcn} is an improvement on the Faster R-CNN approach on benchmarks such as \gls{pascalvoc} and \gls{mscoco}. It is also competitive with the \gls{mscoco} 2015 winning entry \cite{deepres}, while not having any additions such as global context or iterative box regression. Additionally it is considerable faster in training and testing. 
\\\\
The use of \glspl{fcn} are currently the leading method for segmentation both semantic \cite{semfcn} and instance \cite{instancefcn}. With the latter, named \gls{fcis} winning the 2016 \gls{mscoco} instance segmentation challenge and also is the current second place in detection. It uses a similar approach with position-sensitive score maps for pixel-level likelihood for an object category to produce bounding boxes. From these, instance segmentation is performed to produce the pixel-level classification. The main differences between \gls{rfcn} and \gls{fcis} is the addition of ensemble of ResNet models, multi-scale testing and training, and horizontal flipping.