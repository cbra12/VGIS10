\begin{comment}
	- CNN approaches
	- R-CNN - fast - faster
	- SSD
	- R-FCN
	- YOLO (speed)	

	- outline overall CNN methods
		- R-CNN
		- single-shot
	- cites to specific implementations within each
		- inspiration from imagenet / ms coco
\end{comment}

\section{Related Work}
One of the first methods to show that \glspl{cnn} could significantly improve object detection was that of R-CNN \cite{rcnn}. The method obtains the name R-CNN based upon a \gls{cnn} is used on regions of the image. Many earlier object detection approaches were used in a sliding window fashion testing all areas of an image. This can lead to a huge amount of potential testing windows especially if the object detection is done at a multitude of different scales. The method was heavily inspired by the AlexNet model that started the deep learning renaissance in 2012 by winning classification challenge in the \gls{ilsvrc}. The authors of R-CNN aimed to show that the advances in classification with a model such as AlexNet could also be done in object detection. In R-CNN the model is used as a feature extractor from which a class-specific linear \gls{svm} can be trained on top of. The AlexNet-based feature extractor is firstly pre-trained on a large dataset designed for classification, in this case the training set from \gls{ilsvrc} 2012. This pre-trained model is then adapted to the new domain of object detection by fine-tuning the model accordingly. In this instance the authors fine-tuned warped training instances from the \gls{pascalvoc} dataset. The AlexNet model was also altered to classify the 20 classes present in \gls{pascalvoc} rather than the 1000 classes in \gls{ilsvrc}. The pipeline of the R-CNN is split into 3 modules:

\begin{enumerate}
	\item Region proposals.
	\item Feature extraction.
	\item Class-specific linear \glspl{svm}.
\end{enumerate}

In this first module, region proposal, there is a large number of choices of methods to produce a suitable number of windows in comparison to a sliding window approach. R-CNN is agnostic to the region proposal method chosen and in the original work SelectiveSearch \cite{selectivesearch} is used. Module two, as explained earlier, is the use of a \gls{cnn} as a feature extractor. This is in the form of a 4096-dimensional feature vector from the domain-specific \gls{pascalvoc} trained AlexNet model. These feature vectors are used in the third module, class-specific linear \glspl{svm}. In the case of \gls{pascalvoc} a total of 21 \glspl{svm} are trained, one for each of the 20 classes in the challenge and one for a background class. The training of the \glspl{svm} is done by forward propagating a large number of both positive and negative region proposals found with SelectiveSearch and storing each 4096-dimensional feature vector to disk. After this the appropriate labels are applied to each vector and a linear \gls{svm} is optimised for the 21 classes. At test time, for a given image SelectiveSearch is used to produce around 2000 proposals. Each of the proposals are propagated through the network to extract their respective feature vectors. Each feature vector is then tested against every \gls{svm} to produce a score for each class. Finally greedy \gls{nms} is applied to remove overlapping detections. The approach outlined in R-CNN produced a significant improvement in object detection with an improvement of roughly 13\%, compared to previous state-of-the-art methods, to an overall 53.7\% \gls{map} on the \gls{pascalvoc} 2010 test set. Similar results were also found on the \gls{pascalvoc} 2011/12 set with \gls{map} of 53.5\%. Despite the significant improvements with a \gls{cnn}-based method on region proposals there are still issues with the R-CNN. Firstly, the testing time per image is very slow at roughly 47 seconds on an Nvidia K40 GPU. Also extracting features for each proposal in order to train the \glspl{svm} takes a large amount of disk space and may not be feasible on all hardware configurations. Finally, as the R-CNN is made up of 3 modules the training is done in a multi-stage manner rather than end-to-end. Therefore, the loss calculating when optimising the \glspl{svm} are not used to update the \gls{cnn} parameters.
\\\\
The R-CNN method was improved the following year with Fast R-CNN \cite{fastrcnn} and aimed to improve speed and accuracy. One of the significant changes is that the detection training done is now end-end rather than in the multi-stage pipeline in R-CNN. Due to this the large requirements of disk space due to feature caching is no longer required. The Fast-RCNN method takes both an image and a set of pre-computed object proposals. A \gls{cnn} forward propagates the entire image, rather than individual proposals in R-CNN, through several convolutional and max-pooling layers to produce a feature map. Features are extracted for each proposal in their corresponding location in the computed feature map with a \gls{roi} pooling layer. The \gls{roi} feature is calculated by splitting the $h \times w$ proposal into $H \times W$ sub-windows of size $h/H \times w/W$. Where $h$ and $w$ is the height and width respectively of a proposal. $H$ and $W$ are hyper-parameters specifying the fixed spatial extent of the extracted feature. Each sub-window has max-pooling applied and with the resulting value being placed in the corresponding output cell. Once the \gls{roi} pooling layer has been applied to a pre-computed object proposal the forward pass continues through two fully-connected layers followed by two sibling output layers. The sibling outputs are a softmax classification layer that produces probabilities for the object classes and another layer for bounding-box regression. These two layers replace the respective external modules in R-CNN and make it possible to train the entire detection network in a single-stage. As in R-CNN, pre-training a \gls{cnn} on a large classification dataset and fine-tuning towards detection and a specific object classes is done in a similar fashion. In R-CNN, the only deep network used was AlexNet \cite{alexnet}, however, in Fast R-CNN the authors experiment with networks of different size. It was found that the deeper network VGG16 \cite{vgg16} for computing the convolutional feature map gave a considerable improvement in \gls{map}. For a fair comparison of results against R-CNN, its \gls{cnn} was the same pre-trained VGG16 network. The \gls{map} results on \gls{pascalvoc} 2007, 2010, and 2012 compared against R-CNN can be seen in \tableref{fastresults}. 

\begin{table}[]
\centering
\caption{A comparison of R-CNN and Fast R-CNN \gls{pascalvoc} \gls{map} results on the test set from 2007, 2010, and 2012.}
\label{tab:fastresults}
\begin{tabular}{|l|l|l|l|}
 \hline
           & 2007 & 2010 & 2012 \\ \hline
R-CNN      & 66.0 & 62.9 & 62.4 \\ 
Fast R-CNN & \textbf{66.9} & \textbf{66.1} & \textbf{65.7} \\ \hline
\end{tabular}
\end{table}

However, as the name Fast R-CNN implies the main improvement is the speed in respect to both training and testing. By computing a convolutional feature map for an entire image rather than per object proposal the number of passes in the network is lowered significantly. Also by makign the detection training end-to-end with the two sibling layers lowers the training time needed. An overview of time spent training and testing both Fast R-CNN and R-CNN with a VGG16 \gls{cnn} can be seen in \tableref{fastspeed}.

\begin{table}[]
\centering
\caption{Speedup between R-CNN and Fast R-CNN in regards to both training and testing. Both methods are train a VGG16 network for object detection.}
\label{tab:fastspeed}
\begin{tabular}{|l|l|l|}
\hline
                          & R-CNN & Fast R-CNN \\ \hline
Train time (hours)        & 84    & \textbf{9.5}        \\ 
Train speed-up             & 1x    & \textbf{8.8$\times$}       \\ \hline
Test time (seconds/image) & 47.0  & \textbf{0.32}       \\ 
Test speed-up              & 1x    & \textbf{146$\times$}       \\ \hline
\end{tabular}
\end{table}

While Fast R-CNN provided improvements in both accuracy and speed, the increase in speed is only in relation to the actual object detection and assumes that the region proposals are pre-computed. Therefore, there is still a significant bottleneck per image as a region proposal method can typically take a couple of seconds. 
\\\\
The region proposal bottleneck was addressed in the third iteration of the R-CNN network with Faster R-CNN \cite{fasterrcnn}. In this method it was shown how to compute region proposals with a deep \gls{cnn} with a part of the network called \gls{rpn}. A \gls{rpn} shares the convolutional layers and feature map used for computing features with \gls{roi} pooling in Fast R-CNN. As this deep network is already being computed on the entire image for the classification pipeline the added time for proposals using the \gls{rpn} is negligible (10ms) in comparison to a method such as SelectiveSearch. Apart from the change in how region proposals are computed there is no change in comparison to Fast R-CNN. 
An \gls{rpn} takes the convolutional feature map as input and returns a number of object proposals. Each proposals is fed into two sibling layers, similar to that in Fast R-CNN, one layer scoring how likely to be an object or background and another performing bounding-box regression. The proposals are found through a method denoted as anchors. At each sliding-window location $k$ proposals are with anchors that are user-defined reference boxes for how an object proposal may be formed. The anchors can be built based upon scale and aspect ratio altering the size. In the original work three different scales and three aspect ratios are built, yielding a total of $k$ = 9 anchors at each sliding window position. These anchors are then placed on the feature map and the sibling layers calculate the likelihood of an object and regress the anchor as necessary.  
Once the proposals have been found with the \gls{rpn} these are placed on the same convolutional feature map as earlier and the rest of the pipeline is identical to Fast R-CNN, classifying and regressing bounding-boxes with another set of sibling layers. As the only change is the addition of computing proposals in the network with \gls{rpn}, the results are similar in respect to \gls{map}. Only a slight improvement in made on \gls{pascalvoc} 2007 and 2012, from 66.9\% to 69.9\% and 65.7\% to 67.0\% respectively. However, the main contribution to the work is the speed-up of the entire object detection pipeline as the object proposal time is now minimal. On average processing an image on \gls{pascalvoc} 2007 with an Nvidia K40 with Fast R-CNN including proposals took 2 seconds per image. While in Faster R-CNN with the same hardware takes 0.2 seconds per image. A speed-up of 10$\times$ from Fast R-CNN to Faster R-CNN and 250$\times$ from the original R-CNN. The Faster R-CNN methods has also proved to be the foundation for the winning entry in multiple detection challenges including \gls{mscoco}. The results for this challenge with a VGG-16 model for Fast R-CNN were 35.9\% \gls{map}@0.5 \gls{iou} and 19.7\% \gls{map}@[0.5, 0.95]. Faster R-CNN improved this to 42.7\% \gls{map}@0.5 and 21.9\% \gls{map}@[0.5, 0.95].